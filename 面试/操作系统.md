进程、线程

- 进程和线程的区别
- 进程有哪几种状态?
- 进程间的通信方式
- 线程间的同步的方式
- PCB
- 进程的调度算法
- 什么是死锁？死锁的四个必要条件，解决死锁的方法

内存管理

- 常见的内存管理机制
- 内存碎片
- 分段机制和分页机制的区别和共同点
- 分段机制和分页机制下的地址翻译过程分别是怎样的
- 单级页表有什么问题？为什么需要多级页表？
- TLB 有什么用？使用 TLB 之后的地址翻译流程是怎样的？
- 页缺失，常见的页面置换算法有哪些?

文件系统

- 硬链接和软链接有什么区别？
- 常见的磁盘调度算法有哪些？
## 进程管理

1. 一个进程中可以有多个线程，多个线程共享进程的**堆**和**方法区**资源，每个线程有自己的**程序计数器**、**虚拟机栈** 和 **本地方法栈**。
2. 线程和进程最大的不同在于基本上各进程是独立的，而同一进程中的线程极有可能会相互影响。
3. 线程执行开销小，但不利于资源的管理和保护；而进程正相反。
4. 进程切换是一个开销很大的操作，线程切换的成本较低。 

### 进程的状态

-   **创建状态(new)**：进程正在被创建，尚未到就绪状态。
-   **就绪状态(ready)**：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。
-   **运行状态(running)**：进程正在处理器上运行(单核 CPU 下任意时刻只有一个进程处于运行状态)。
-   **阻塞状态(waiting)**：又称为等待状态，进程正在等待某一事件而暂停运行，如等待某资源或等待 IO 操作完成。
-   **结束状态(terminated)**：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。

![进程状态图转换图](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/state-transition-of-process.png)

### 僵尸进程、孤儿进程

1. **僵尸进程**

僵尸进程是已完成且处于终止状态，但在进程表中却仍然存在的进程。

僵尸进程一般发生有父子关系的进程中，一个子进程的进程描述符在子进程退出时不会释放，只有当父进程通过 wait() 或 waitpid() 获取了子进程信息后才会释放。如果子进程退出，而父进程并没有调用 wait() 或 waitpid()，那么子进程的进程描述符仍然保存在系统中。

2. **孤儿进程**

一个父进程退出，而它的一个或多个子进程还在运行，那么这些子进程将成为孤儿进程。孤儿进程将被 init 进程 (进程 ID 为 1 的进程) 所收养，并由 init 进程对它们完成状态收集工作。因为孤儿进程会被 init 进程收养，所以孤儿进程不会对系统造成危害。

### PCB 

**PCB（Process Control Block）** 即进程控制块，是操作系统中用来管理和跟踪进程的数据结构，每个进程都对应着一个独立的 PCB。你可以将 PCB 视为进程的大脑。

当操作系统创建一个新进程时，会为该进程分配一个唯一的进程 ID，并且为该进程创建一个对应的进程控制块。当进程执行时，PCB 中的信息会不断变化，操作系统会根据这些信息来管理和调度进程。

PCB 主要包含下面几部分的内容：

-   进程的描述信息，包括进程的名称、标识符等等；
-   进程的调度信息，包括进程阻塞原因、进程状态（就绪、运行、阻塞等）、进程优先级（标识进程的重要程度）等等；
-   进程对资源的需求情况，包括 CPU 时间、内存空间、I/O 设备等等。
-   进程打开的文件信息，包括文件描述符、文件类型、打开模式等等。

### 调度算法

-   **先到先服务调度算法** 
	
	非抢占式的调度算法，按照请求的顺序进行调度。有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。另外，对I/O密集型进程也不利，因为这种进程每次进行I/O操作之后又得重新排队。
	
-   **短作业优先的调度算法** 
	
	非抢占式的调度算法，按估计运行时间最短的顺序进行调度。长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。
	
-   **时间片轮转调度算法**
	
	每个进程被分配一个时间段，称作它的时间片，进程只能在自己的时间片内运行。
	
-  **优先级调度算法**
	
	为每个进程分配一个优先级，按优先级进行调度。为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。
	
-   **多级反馈队列调度算法**：
	
	最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。

### 通信方式

1. 管道：
	
	所谓的管道就是内核中的一串缓存，从管道的一端写入数据，就是缓存在了内核里，另一端读取，也是从内核中读取这段数据。
	
	管道可以分为两类：**匿名管道**和**命名管道**。匿名管道是单向的，只能在有亲缘关系的进程间通信；命名管道是双向的，可以实现本机任意两个进程通信。

2. 信号 ： 
	
	发送方发送内容并指定接收的进程，然后发出特定的中断，操作系统接到中断请求后，找到接收进程，通知接收进程处理信号。
    
    比如`kill -9 1050`就表示给PID为1050的进程发送`SIGKIL`信号。

3. 消息队列：
	
	**消息队列就是保存在内核中的消息链表**，包括Posix消息队列和System V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。

4. 共享内存：
	
	共享内存的机制，就是拿出⼀块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写⼊的东西，另外的进程⻢上就能看到。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量，配合使用，来实现进程间的同步和通信。

5. 信号量：
	
	**它本质上是一个整数计数器**，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。
	
	信号量表示资源的数量，控制信号量的⽅式有两种原⼦操作：
	
	- ⼀个是 **P** **操作**，这个操作会把信号量减去 1，相减后如果信号量 < 0，则表明资源已被占⽤，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使⽤，进程可正常继续执⾏。
	- 另⼀个是 **V** **操作**，这个操作会把信号量加上 1，相加后如果信号量 <= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运⾏；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；

6. Socket：
	
	与其他通信机制不同的是，它可用于不同机器间的进程通信。

7. 总结

- 管道：简单；效率低，容量有限；
- 消息队列：不及时，写入和读取需要用户态、内核态拷贝。
- 共享内存区：能够很容易控制容量，速度快，但需要注意不同进程的同步问题。
- 信号量：不能传递复杂消息，一般用来实现进程间的同步；
- 信号：它是进程间通信的唯一异步机制。
- Socket：用于不同主机进程间的通信。

### 同步方式

同步解决的多线程/进程操作共享资源的问题，目的是不管线程/进程之间的执行如何穿插，最后的结果都是正确的。

**临界区**：对共享资源访问的程序片段称为临界区，这段代码应该是互斥的。

临界区不仅针对线程，同样针对进程。

临界区同步的一些实现方式：

1、**锁**

任何想进⼊临界区的线程，必须先执⾏加锁操作。若加锁操作顺利通过，则线程可进⼊临界区；在完成对临界资源的访问后再执⾏解锁操作，以释放该临界资源。

加锁和解锁锁住的对象可以是`临界区对象`，也可以只是一个简单的`互斥量`。

2、**信号量**

信号量是操作系统提供的⼀种协调共享资源访问的⽅法。

通常**信号量表示资源的数量**，对应的变量是⼀个整型变量。

另外，还有**两个原⼦操作的系统调⽤函数来控制信号量的**，分别是：

- _P_ 操作：将 sem 减 1 ，相减后，如果 sem < 0 ，则进程/线程进⼊阻塞等待，否则继续，表明 P操作可能会阻塞；
    
- _V_ 操作：将 sem 加 1 ，相加后，如果 sem <= 0 ，唤醒⼀个等待中的进程/线程，表明 V 操作不会阻塞；

### 死锁

死锁的四个必要条件

1.  **互斥**：资源必须处于非共享模式，即一次只有一个进程可以使用。如果另一进程申请该资源，那么必须等待直到该资源被释放为止。
2.  **占有并等待**：一个进程至少应该占有一个资源，并等待另一资源，而该资源被其他进程所占有。
3.  **非抢占**：资源不能被抢占。只能在持有资源的进程完成任务后，该资源才会被释放。
4.  **循环等待**：有一组等待进程 `{P0, P1,..., Pn}`， `P0` 等待的资源被 `P1` 占有，`P1` 等待的资源被 `P2` 占有，......，`Pn-1` 等待的资源被 `Pn` 占有，`Pn` 等待的资源被 `P0` 占有。

**解决方式**

1. **消除互斥条件**

这个是没法实现，因为很多资源就是只能被一个线程占用，例如锁。

2. **消除请求并持有条件**

一个线程一次请求其所需要的所有资源。

3. **消除不可剥夺条件**

占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。

4. **消除环路等待条件**

可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后就不存在环路了。

## 内存管理

### 内存管理方式

内存管理方式可以简单分为下面两种：

-   **连续内存管理**：为一个用户程序分配一个连续的内存空间，内存利用率一般不高。
-   **非连续内存管理**：允许一个程序使用的内存分布在离散或者说不相邻的内存中，相对更加灵活一些。

**1.连续内存管理**

- **块式管理** 
- **伙伴系统**

**2.非连续内存管理**

-   **段式管理**
-   **页式管理**
-   **段页式管理**

**内存碎片**

-   **内部内存碎片(简称内部碎片)**：已经分配给进程使用但未被使用的内存。当采用固定比例比如 2 的幂次方进行内存分配时，进程所分配的内存可能会比其实际所需要的大。
-   **外部内存碎片(简称为外部碎片)**：由于未分配的连续内存区域太小，以至于不能满足任意进程所需要的内存分配请求，这些小片段且不连续的内存空间被称为外部碎片。

![内存碎片](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/internal-and-external-fragmentation.png)

### 分段机制

**分段机制** 以段(—段 **连续** 的物理内存)的形式管理/分配物理内存。应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的，每个段定义了一组逻辑信息，例如有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等。

分段管理通过 **段表** 映射虚拟地址和物理地址。

分段机制下的虚拟地址由两部分组成：

-   **段号**
-   **段内偏移量**

具体的地址翻译过程如下：

1.  CPU中的内存管理单元(MMU) 首先解析得到虚拟地址中的段号；
2.  通过段号去该应用程序的段表中取出对应的段信息；
3.  从段信息中取出该段的起始地址（物理地址）加上虚拟地址中的段内偏移量得到最终的物理地址。

![分段机制下的地址翻译过程](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/segment-virtual-address-composition.png)

段表中还存有诸如段长、段类型 等信息。

分段机制容易出现外部内存碎片

举个例子：假设可用物理内存为 5G 的系统使用分段机制分配内存。现在有 4 个进程，每个进程的内存占用情况如下：

-   进程 1：0~1G（第 1 段）
-   进程 2：1~3G（第 2 段）
-   进程 3：3~4.5G（第 3 段）
-   进程 4：4.5~5G（第 4 段）

此时，我们关闭了进程 1 和进程 4，则第 1 段和第 4 段的内存会被释放，空闲物理内存还有 1.5G。由于这 1.5G 物理内存并不是连续的，导致没办法将空闲的物理内存分配给一个需要 1.5G 物理内存的进程。

![分段机制导致外部内存碎片](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/segment-external-memory-fragmentation.png)

### 分页机制

**分页机制** 把主存（物理内存）分为连续等长的物理页，应用程序的虚拟地址空间划也被分为连续等长的虚拟页。现代操作系统广泛采用分页机制。

**注意：这里的页是连续等长的，不同于分段机制下不同长度的段。**

在分页机制下，应用程序虚拟地址空间中的任意虚拟页可以被映射到物理内存中的任意物理页上，因此可以实现物理内存资源的离散分配。分页机制按照固定页大小分配物理内存，使得物理内存资源易于管理，可有效避免分段机制中外部内存碎片的问题。

分页管理通过 **页表** 映射虚拟地址和物理地址。

![单级页表](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/page-table.png)

在分页机制下，每个应用程序都会有一个对应的页表。

分页机制下的虚拟地址由两部分组成：

-   **页号**：通过虚拟页号可以从页表中取出对应的物理页号；
-   **页内偏移量**：物理页起始地址+页内偏移量=物理内存地址。

具体的地址翻译过程如下：

1.  CPU中的内存管理单元(MMU)  首先解析得到虚拟地址中的虚拟页号；
2.  通过虚拟页号去该应用程序的页表中取出对应的物理页号；
3.  用该物理页号对应的物理页起始地址加上虚拟地址中的页内偏移量得到最终的物理地址。

![分页机制下的地址翻译过程](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/paging-virtual-address-composition.png)

页表中还存有诸如访问标志、页类型等信息。

可能会存在 **页缺失** 。物理内存中没有对应的物理页或者对应的页表项不存在。

**TLB**

为了提高虚拟地址到物理地址的转换速度，操作系统在 **页表方案** 基础之上引入了 **转址旁路缓存(Translation Lookasjde Buffer，TLB，也被称为快表)** 。

![加入 TLB 之后的地址翻译](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/physical-virtual-address-translation-mmu.png)

TLB 本质上就是一块高速缓存（Cache），缓存了虚拟页号到物理页号的映射关系，你可以将其简单看作是存储着键（虚拟页号）值（物理页号）对的哈希表。

使用 TLB 之后的地址翻译流程是这样的：

1.  用虚拟地址中的虚拟页号作为 key 去 TLB 中查询；
2.  如果能查到对应的物理页的话，就不用再查询页表了，这种情况称为 TLB 命中（TLB hit)。
3.  如果不能查到对应的物理页的话，还是需要去查询主存中的页表，同时将页表中的该映射表项添加到 TLB 中，这种情况称为 TLB 未命中（TLB miss)。
4.  当 TLB 填满后，又要登记新页时，就按照一定的淘汰策略淘汰掉快表中的一个页。

由于页表也在主存中，因此在没有 TLB 之前，每次读写内存数据时 CPU 要访问两次主存。有了 TLB 之后，对于存在于 TLB 中的页表数据只需要访问一次主存即可。

TLB 的设计思想非常简单，但命中率往往非常高，效果很好。这就是因为被频繁访问的页就是其中的很小一部分。

### 分页机制 VS 分段机制

**共同点**：

-   都是非连续内存管理的方式。
-   都采用了地址映射的方法，将虚拟地址映射到物理地址，以实现对内存的管理和保护。

**区别**：

-   分页机制以页面为单位进行内存管理，而分段机制以段为单位进行内存管理。页的大小是固定的，由操作系统决定，通常为 2 的幂次方。而段的大小不固定，取决于我们当前运行的程序。
-   分段机制容易出现外部内存碎片。分页机制解决了外部内存碎片的问题，但仍然可能会出现内部内存碎片。
-   分页机制采用了页表来完成虚拟地址到物理地址的映射，页表通过一级页表和二级页表来实现多级映射；而分段机制则采用了段表来完成虚拟地址到物理地址的映射，每个段表项中记录了该段的起始地址和长度信息。

### 页缺失

常见的页缺失有下面这两种：

-   **硬性页缺失（Hard Page Fault）**：物理内存中没有对应的物理页。于是，Page Fault Handler 会指示 CPU 从已经打开的磁盘文件中读取相应的内容到物理内存，而后交由 MMU 建立相应的虚拟页和物理页的映射关系。
-   **软性页缺失（Soft Page Fault）**：物理内存中有对应的物理页，但虚拟页还未和物理页建立映射。于是，Page Fault Handler 会指示 MMU 建立相应的虚拟页和物理页的映射关系。

发生上面这两种缺页错误的时候，应用程序访问的是有效的物理内存，只是出现了物理页缺失或者虚拟页和物理页的映射关系未建立的问题。如果应用程序访问的是无效的物理内存的话，还会出现 **无效缺页错误（Invalid Page Fault）** 。

### 页面置换算法

当发生硬性页缺失时，如果物理内存中没有空闲的物理页面可用的话。操作系统就必须将物理内存中的一个物理页淘汰出去，这样就可以腾出空间来加载新的页面了。

1.  **最佳页面置换算法（OPT，Optimal）**：优先选择淘汰的页面是以后永不使用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率。但由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现，只是理论最优的页面置换算法，可以作为衡量其他置换算法优劣的标准。
2.  **先进先出页面置换算法（FIFO，First In First Out）** : 最简单的一种页面置换算法，总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。该算法易于实现和理解，一般只需要通过一个 FIFO 队列即可需求。不过，它的性能并不是很好。
3.  **最近最久未使用页面置换算法（LRU ，Least Recently Used）**：LRU 算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。LRU 算法是根据各页之前的访问情况来实现，因此是易于实现的。OPT 算法是根据各页未来的访问情况来实现，因此是不可实现的。
4.  **最少使用页面置换算法（LFU，Least Frequently Used）** : 和 LRU 算法比较像，不过该置换算法选择的是之前一段时间内使用最少的页面作为淘汰页。
5.  **时钟页面置换算法（Clock）**：可以认为是一种最近未使用算法，即逐出的页面都是最近没有使用的那个。

LRU 算法是实际使用中应用的比较多，也被认为是最接近 OPT 的页面置换算法。

### 局部性原理

局部性原理是指在程序执行过程中，数据和指令的访问存在一定的空间和时间上的局部性特点。其中，时间局部性是指一个数据项或指令在一段时间内被反复使用的特点，空间局部性是指一个数据项或指令在一段时间内与其相邻的数据项或指令被反复使用的特点。

-   **时间局部性**：由于程序中存在一定的循环或者重复操作，因此会反复访问同一个页或一些特定的页，为了利用时间局部性，分页机制中通常采用缓存机制来提高页面的命中率，即将最近访问过的一些页放入缓存中，如果下一次访问的页已经在缓存中，就不需要再次访问内存，而是直接从缓存中读取。
-   **空间局部性**：当访问某个页时，往往会顺带访问其相邻的一些页。为了利用空间局部性，分页机制中通常采用预取技术来预先将相邻的一些页读入内存缓存中，以便在未来访问时能够直接使用，从而提高访问速度。

## 外存管理

### 硬链接、软链接

**1、硬链接（Hard Link）**

-   在 Linux/类 Unix 文件系统中，每个文件和目录都有一个唯一的索引节点（inode）号，用来标识该文件或目录。硬链接通过 inode 节点号建立连接，硬链接和源文件的 inode 节点号相同，两者对文件系统来说是完全平等的（可以看作是互为硬链接，源头是同一份文件），删除其中任何一个对另外一个没有影响，可以通过给文件设置硬链接文件来防止重要文件被误删。
-   只有删除了源文件和所有对应的硬链接文件，该文件才会被真正删除。
-   硬链接具有一些限制，不能对目录以及不存在的文件创建硬链接，并且，硬链接也不能跨越文件系统。
-   `ln` 命令用于创建硬链接。
  
![硬链接-来源参考[3]](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-d3f778f9-506b-4b93-9fb7-40eb0a79874e.png)

**2、软链接（Symbolic Link 或 Symlink）**

-   软链接和源文件的 inode 节点号不同。
-   源文件删除后，软链接依然存在，但是指向的是一个无效的文件路径。
-   软连接类似于 Windows 系统中的快捷方式。
-   不同于硬链接，可以对目录或者不存在的文件创建软链接，并且，软链接可以跨越文件系统。
-   `ln -s` 命令用于创建软链接。

![软链接-来源参考[3]](https://cdn.tobebetterjavaer.com/tobebetterjavaer/images/sidebar/sanfene/os-81abf13c-5c60-4263-8fcb-c79c33d865e8.png)

### 磁盘调度算法

磁盘调度算法是操作系统中对磁盘访问请求进行排序和调度的算法，其目的是提高磁盘的访问效率。

一次磁盘读写操作的时间由磁盘寻道/寻找时间、延迟时间和传输时间决定。磁盘调度算法可以通过改变到达磁盘请求的处理顺序，减少磁盘寻道时间和延迟时间。

1.  **先来先服务算法（First-Come First-Served，FCFS）**：按照请求到达磁盘调度器的顺序进行处理，先到达的请求的先被服务。FCFS 算法实现起来比较简单，不存在算法开销。不过，由于没有考虑磁头移动的路径和方向，平均寻道时间较长。同时，该算法容易出现饥饿问题，即一些后到的磁盘请求可能需要等待很长时间才能得到服务。
2.  **最短寻道时间优先算法（Shortest Seek Time First，SSTF）**：也被称为最佳服务优先（Shortest Service Time First，SSTF）算法，优先选择距离当前磁头位置最近的请求进行服务。SSTF 算法能够最小化磁头的寻道时间，但容易出现饥饿问题，即磁头附近的请求不断被服务，远离磁头的请求长时间得不到响应。实际应用中，需要优化一下该算法的实现，避免出现饥饿问题。
3.  **扫描算法（SCAN）**：也被称为电梯（Elevator）算法，基本思想和电梯非常类似。磁头沿着一个方向扫描磁盘，如果经过的磁道有请求就处理，直到到达磁盘的边界，然后改变移动方向，依此往复。SCAN 算法能够保证所有的请求得到服务，解决了饥饿问题。但是，如果磁头从一个方向刚扫描完，请求才到的话。这个请求就需要等到磁头从相反方向过来之后才能得到处理。
4.  **循环扫描算法（Circular Scan，C-SCAN）**：SCAN 算法的变体，只在磁盘的一侧进行扫描，并且只按照一个方向扫描，直到到达磁盘边界，然后回到磁盘起点，重新开始循环。
5.  **边扫描边观察算法（LOOK）**：SCAN 算法中磁头到了磁盘的边界才改变移动方向，这样可能会做很多无用功，因为磁头移动方向上可能已经没有请求需要处理了。LOOK 算法对 SCAN 算法进行了改进，如果磁头移动方向上已经没有别的请求，就可以立即改变磁头移动方向，依此往复。也就是边扫描边观察指定方向上还有无请求，因此叫 LOOK。
6.  **均衡循环扫描算法（C-LOOK）**：C-SCAN 只有到达磁盘边界时才能改变磁头移动方向，并且磁头返回时也需要返回到磁盘起点，这样可能会做很多无用功。C-LOOK 算法对 C-SCAN 算法进行了改进，如果磁头移动的方向上已经没有磁道访问请求了，就可以立即让磁头返回，并且磁头只需要返回到有磁道访问请求的位置即可。